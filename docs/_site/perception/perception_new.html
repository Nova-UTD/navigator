<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/navigator/assets/css/just-the-docs-default.css"> <script src="/navigator/assets/js/vendor/lunr.min.js"></script> <script src="/navigator/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Perception and Prediction Design Document (draft) | Navigator</title> <meta name="generator" content="Jekyll v3.9.2" /> <meta property="og:title" content="Perception and Prediction Design Document (draft)" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Navigator is an open-source autonomous driving system developed by Nova, an applied research group at UT Dallas." /> <meta property="og:description" content="Navigator is an open-source autonomous driving system developed by Nova, an applied research group at UT Dallas." /> <link rel="canonical" href="http://0.0.0.0:8083/navigator/perception/perception_new.html" /> <meta property="og:url" content="http://0.0.0.0:8083/navigator/perception/perception_new.html" /> <meta property="og:site_name" content="Navigator" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Perception and Prediction Design Document (draft)" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Navigator is an open-source autonomous driving system developed by Nova, an applied research group at UT Dallas.","headline":"Perception and Prediction Design Document (draft)","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://0.0.0.0:8083/navigator/assets/res/logo.png"}},"url":"http://0.0.0.0:8083/navigator/perception/perception_new.html"}</script> <!-- End Jekyll SEO tag --> <link rel="shortcut icon" type="image/png" href="/navigator/assets/res/favicon.png"> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header"> <a href="/navigator/" class="site-title lh-tight"> <div class="site-logo"></div> </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/navigator/system-overview.html" class="nav-list-link">System overview</a></li><li class="nav-list-item"><a href="/navigator/sensing/sensing-overview.html" class="nav-list-link">Sensing</a></li><li class="nav-list-item"><a href="/navigator/perception/segmentation.html" class="nav-list-link">Semantic segmentation</a></li><li class="nav-list-item"><a href="/navigator/perception/mapping.html" class="nav-list-link">Localization and Mapping</a></li><li class="nav-list-item"><a href="/navigator/perception/perception-overview.html" class="nav-list-link">Perception</a></li><li class="nav-list-item"><a href="/navigator/contributing/tricks.html" class="nav-list-link">Tricks</a></li><li class="nav-list-item"><a href="/navigator/controls/controls-overview.html" class="nav-list-link">Controls</a></li><li class="nav-list-item"><a href="/navigator/contributing/contributing-overview.html" class="nav-list-link">Contributing</a></li><li class="nav-list-item"><a href="/navigator/simulation/simulation-overview.html" class="nav-list-link">Simulation</a></li><li class="nav-list-item"><a href="/navigator/contributing/troubleshooting.html" class="nav-list-link">Troubleshooting</a></li><li class="nav-list-item"><a href="/navigator/interface/schematics.html" class="nav-list-link">Schematics</a></li><li class="nav-list-item"><a href="/navigator/interface/interface-overview.html" class="nav-list-link">Interface</a></li><li class="nav-list-item"><a href="/navigator/planning/airbags.html" class="nav-list-link">Airbags</a></li><li class="nav-list-item"><a href="/navigator/planning/design.html" class="nav-list-link">Behavior Planning and Controls Design Document (draft)</a></li><li class="nav-list-item"><a href="/navigator/planning/maps.html" class="nav-list-link">Maps</a></li><li class="nav-list-item"><a href="/navigator/opendrivepy.html" class="nav-list-link">OpenDrivePy</a></li><li class="nav-list-item active"><a href="/navigator/perception/perception_new.html" class="nav-list-link active">Perception and Prediction Design Document (draft)</a></li><li class="nav-list-item"><a href="/navigator/planning/planning-overview.html" class="nav-list-link">Planning</a></li><li class="nav-list-item"><a href="/navigator/writing-documentation.html" class="nav-list-link">Writing documentation</a></li></ul> <ul class="nav-list"><li class="nav-list-item external"> <a href="https://nova-utd.github.io" class="nav-list-link external"> Learn about Nova <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg> </a> </li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Navigator" aria-label="Search Navigator" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="//nova-utd.github.io" class="site-button" > Learn about Nova </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <div id="main-content" class="main-content" role="main"> <h1 class="no_toc" id="perception-and-prediction-design-document-draft"> <a href="#perception-and-prediction-design-document-draft" class="anchor-heading" aria-labelledby="perception-and-prediction-design-document-draft"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Perception and Prediction Design Document (draft) </h1> <p><em>Maintained by Ashwin</em></p> <h2 class="no_toc text-delta" id="table-of-contents"> <a href="#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Table of contents </h2> <ol id="markdown-toc"> <li><a href="#system-overview" id="markdown-toc-system-overview">System Overview</a> <ol> <li><a href="#current-system" id="markdown-toc-current-system">Current System:</a> <ol> <li><a href="#prediction" id="markdown-toc-prediction">Prediction:</a></li> </ol> </li> <li><a href="#planned-system" id="markdown-toc-planned-system">Planned System:</a> <ol> <li><a href="#perception" id="markdown-toc-perception">Perception:</a></li> <li><a href="#prediction-1" id="markdown-toc-prediction-1">Prediction:</a></li> </ol> </li> <li><a href="#proposed-structures-for-phase-1" id="markdown-toc-proposed-structures-for-phase-1">Proposed structures for Phase 1:</a> <ol> <li><a href="#3d2d-bounding-boxes" id="markdown-toc-3d2d-bounding-boxes">3D/2D bounding boxes:</a></li> <li><a href="#system-overview-for-dynamic-environment-prediction" id="markdown-toc-system-overview-for-dynamic-environment-prediction">System overview for Dynamic Environment Prediction</a></li> <li><a href="#proposed-structure-for-dynamic-environment-prediction" id="markdown-toc-proposed-structure-for-dynamic-environment-prediction">Proposed structure for Dynamic Environment Prediction</a></li> </ol> </li> </ol> </li> </ol><hr /> <h2 id="system-overview"> <a href="#system-overview" class="anchor-heading" aria-labelledby="system-overview"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> System Overview </h2> <h3 id="current-system"> <a href="#current-system" class="anchor-heading" aria-labelledby="current-system"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Current System: </h3> <h4 id="prediction"> <a href="#prediction" class="anchor-heading" aria-labelledby="prediction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Prediction: </h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Zones in place of dynamic object detection 

- Acts as a safety net cast over objects in view of the vehicle 

- Car "reacts" to zone/wall proximity and zone speed argument 

- Slows down to minimum speed argument, regardless if there are multiple zones 

- Primitive 

- Unable to further expand implementation 

- Zone algorithm highlighted more in depth in behavior planning and control document 
</code></pre></div></div> <h3 id="planned-system"> <a href="#planned-system" class="anchor-heading" aria-labelledby="planned-system"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Planned System: </h3> <h4 id="perception"> <a href="#perception" class="anchor-heading" aria-labelledby="perception"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Perception: </h4> <p>Phase 1:</p> <ul> <li> <p>3D/2D bounding boxes (tracked)</p> </li> <li> <p>Static occupancy grid</p> </li> </ul> <p>Phase 2:</p> <ul> <li> <p>Landmark detection (Stop lights priority)</p> </li> <li> <p>Semantic encoding</p> </li> <li> <p>Map masks: Stop lines, Cross walks</p> </li> </ul> <p>Phase 3:</p> <ul> <li> <p>Tune models within simulator</p> </li> <li> <p>Eventually deploy and test on vehicle</p> </li> </ul> <h4 id="prediction-1"> <a href="#prediction-1" class="anchor-heading" aria-labelledby="prediction-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Prediction: </h4> <p>Phase 1:</p> <ul> <li> <p>Physics-based prediction implemented</p> </li> <li> <p>Treats cars like particles (acceleration &amp; velocity)</p> </li> <li> <p>Dynamic occupancy grid</p> </li> </ul> <p>Phase 2:</p> <ul> <li> <p>ML-based occupancy grid (from physics-based)</p> </li> <li> <p>Non-sequential learning</p> </li> <li> <p>Sequential learning</p> </li> <li> <p>Recurrent Neural Nets</p> </li> </ul> <p>Dynamic Bayesian Network</p> <p>Phase 3:</p> <ul> <li> <p>ML refined</p> </li> <li> <p>Map priors</p> </li> <li> <p>Deployed and tested on vehicle</p> </li> </ul> <p><strong>**</strong>** Insert first image <em>**</em></p> <p><img src="/navigator/assets/res/perception_flow.jpeg" alt="perception_flowchart" /></p> <h3 id="proposed-structures-for-phase-1"> <a href="#proposed-structures-for-phase-1" class="anchor-heading" aria-labelledby="proposed-structures-for-phase-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Proposed structures for Phase 1: </h3> <h4 id="3d2d-bounding-boxes"> <a href="#3d2d-bounding-boxes" class="anchor-heading" aria-labelledby="3d2d-bounding-boxes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3D/2D bounding boxes: </h4> <p>3D:</p> <p>Estimating 3D bounding boxes based on 2D images:</p> <p>A method for 3D object detection using 2D images. This method involves utilizing a deep convolutional network to obtain relatively stable 3D object properties, and utilize this along with some geometric constraints provided by a 2d bounding box (would probably need to figure this out using something like YOLOv3) in order to construct a 3D bounding box for the corresponding object.</p> <p>Given the pose of the object in the camera coordinate frame (R, T) ∈ SE(3) and the camera intrinsic matrix K, the projection of a 3D point Xo = [X, Y, Z, 1]^T in the object’s coordinate frame into the image x = [x, y, 1]T is:<br /> x = K [R T] Xo [1]</p> <p>2D:</p> <p>2D object detection from a BEV representation (Bird’s Eye View, looking down from the Z axis) of our pseudo-lidar point cloud data may be more efficient than processing a generic 3D convolution. This architecture may also include semantic segmentation, effectively enabling us to accomplish landmark detection as well without any additional computation.</p> <p>Can be a solution for stereo-images.</p> <p>Depth correction w/ existing LiDAR sensors</p> <p>2D BEVDetNet Object Detection</p> <p>Pseudo-LiDAR</p> <h4 id="system-overview-for-dynamic-environment-prediction"> <a href="#system-overview-for-dynamic-environment-prediction" class="anchor-heading" aria-labelledby="system-overview-for-dynamic-environment-prediction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> System overview for Dynamic Environment Prediction </h4> <p>Our system uses point clouds for dynamic environment prediction. We use a ConvLSTM architecture intended for video frame prediction to instead predict the local environment surrounding an autonomous agent across future time steps. For this purpose, we adapt the PredNet architecture designed for video frame prediction in autonomous driving scenes. The ConvLSTM is expected to learn an internal representation of the dynamics within the local environment from occupancy grid data. The grid inputs are generated from LiDAR measurement recordings in the KITTI dataset taken across a variety of urban scenes [1].</p> <h4 id="proposed-structure-for-dynamic-environment-prediction"> <a href="#proposed-structure-for-dynamic-environment-prediction" class="anchor-heading" aria-labelledby="proposed-structure-for-dynamic-environment-prediction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Proposed structure for Dynamic Environment Prediction </h4> <p><img src="/navigator/assets/res/perception_flow_prednet.png" alt="dep_flow" /></p> <p>Inputs for Dynamic Environment Prediction</p> <ul> <li>Point Cloud</li> </ul> <p>Produces for Dynamic Environment Prediction</p> <ul> <li> <p>Ground Segmentation</p> </li> <li> <p>Occupancy grid and DOGMA</p> </li> <li> <p>PredNet (Neural Network Architecture)</p> </li> </ul> <p>Dynamic Environment Prediction does not handle…</p> <ul> <li>More research needed…</li> </ul> <p>Dynamic Environment Prediction: Ground Segmentation</p> <p>Prior to generating an occupancy grid, the ground must be segmented and removed from the LiDAR point cloud. Markov Random Field (MRF) algorithm that exploits local spatial relationships, avoiding the assumption of a planar ground [2][1].</p> <p>Dynamic Environment Prediction: Dynamic Occupancy Grid Maps (DOGMas)</p> <p>A DOGMa is an evidential grid containing both occupancy and dynamic state information (e.g., velocity). We generate DOGMas via the procedure outlined by Nuss et al. [4]. There are two parallel processes that occur: occupancy grid updates and cell-wise velocity estimates [1].</p> <p>Occupancy Grids</p> <p>We consider DST-based occupancy grids computed from LiDAR measurements as detailed by Nuss et al. [4]. DST deals with a set of exhaustive hypotheses formed from a frame of discernment and the associated belief masses. In the case of an occupancy grid, our frame of discernment is: Ω = {F, O}, where F is free space and O is occupied space. Thus, the set of hypotheses is: {∅, {F }, {O}, {F, O}}. The null set ∅ is impossible in this context as a cell physically cannot be neither occupied nor unoccupied. Thus, our exhaustive set of hypotheses is: {{F }, {O}, {F, O}}. The sum of the belief masses over the possible hypotheses for each individual cell must equal one by definition, akin to probabilities [1].</p> <p>Velocity Estimation</p> <p>To incorporate dynamics, we estimate the velocity for each cell. The velocity estimates use a DST approximation for a probability hypothesis density filter with multi-instance Bernoulli (PHD/MIB) [5]. DST allows for the particle filter to run more efficiently by initializing particles only in cells with occupied masses above a specified threshold, avoiding occluded regions without measurements[1].</p> <p>Dynamic Environment Prediction: Neural Network Architecture</p> <p>We repurpose the PredNet architecture to learn the spatial and temporal representation of the environment by training it on occupancy grids instead of images. The convolutional layers exploit contextual information to correlate the occupied cells, removing the cell independence assumption [6]. The self-supervised nature of sequential data prediction is advantageous as human-labeled LiDAR data is expensive to obtain [6]. In this framework, the labels are simply the input environment representation (grids) at a later time instance. Although the original PredNet model was designed for video data, we demonstrate that the architecture can be re-used in the LiDAR setting [1].</p> <p>Dynamic Environment Prediction: Experiments</p> <ul> <li> <p>Dataset Generation</p> <ul> <li> <p>LiDAR Measurement Grids</p> </li> <li> <p>The KITTI HDL-64E Velodyne LiDAR dataset was augmented for use in occu- pancy grid prediction [7]. The dataset contains a variety of urban road scenes in Karlsruhe, Germany. We use 35, 417 frames (138 driving sequences) for training, 496 frames (3 driving sequences) for validation, and 2, 024 frames (7 driving sequences) for testing. Velodyne LiDAR point cloud measurements are obtained at 10 Hz.</p> </li> <li> <p>Each LiDAR point cloud is filtered to remove the points corresponding to the ground as described in Section II. Then, a simple form of ray-tracing is performed to determine the free space between a LiDAR measurement and the ego vehicle. Each resulting local grid is centered at the ego vehicle GPS coordinate position. The shorter grid range is acceptable for slower speeds in urban settings, as is the case in the KITTI dataset [7][1].</p> </li> <li> <p>Dynamic Occupancy Grid Maps</p> <ul> <li>Dynamic Occupancy Grid Maps: The DOGMa’s occupancy and velocity information is computed from the LiDAR data as outlined in Section II. The velocities are then filtered to remove static measurements according to the cell-wise Mahalanobis distance: τ = vT P v, where v is the velocity vector and P is the cell’s covariance matrix as computed from the particles [21]. Cells with occupancy masses below a threshold are also removed. The velocities are then normalized to the range [−1, 1] and stacked with either (1) the pignistic probability (Eq. (5)) or (2) the DST mass (Eq. (2) and Eq. (3)) occupancy grids, forming the input to the network. [1]</li> </ul> </li> </ul> </li> <li> <p>PredNet Experiments</p> <ul> <li>PredNet was trained and tested on an NVIDIA GeForce GTX 1070 GPU. At test time, one sequence (15 predictions totaling 1.5 s ahead) took on average 0.1 s to run. [1]</li> </ul> </li> </ul> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
